const OLLAMA_API_KEY = import.meta.env.VITE_OLLAMA_API_KEY;
const OLLAMA_BASE_URL = import.meta.env.VITE_OLLAMA_BASE_URL || "https://api.ollama.cloud";
const OLLAMA_API_URL = `${OLLAMA_BASE_URL}/api/chat`;
const MODEL = "llama3.2"; // ou outro modelo dispon√≠vel no cloud

const systemInstruction = 
  "Seu nome √© Tuba. " +
  "Voc√™ √© um avatar em forma de tubar√£o que participa de debates. " +
  "Voc√™ deve, em tom assertivo, respeitoso e equilibrado, debater com o usu√°rio sobre o tema proposto. " +
  "Analise os argumentos apresentados, ofere√ßa contra-argumentos fundamentados e mantenha o debate produtivo. " +
  "Quando for o √∫ltimo turno do debate, fa√ßa suas considera√ß√µes finais de forma mais concisa e conclusiva. " +
  "Sempre mantenha o respeito e o profissionalismo, mesmo em discord√¢ncias. " +
  "N√£o d√™ respostas muito longas, 1 par√°grafo j√° basta.";

export const sendMessageToOllama = async (message, history = []) => {
  try {
    // Formata o hist√≥rico no formato do Ollama
    const messages = [
      { role: "system", content: systemInstruction },
      ...history.map(msg => ({
        role: msg.role === "model" ? "assistant" : msg.role,
        content: msg.parts[0].text
      })),
      { role: "user", content: message }
    ];

    const headers = {
      "Content-Type": "application/json",
    };

    // Adiciona a API key se estiver usando cloud
    if (OLLAMA_API_KEY) {
      headers["Authorization"] = `Bearer ${OLLAMA_API_KEY}`;
    }

    const response = await fetch(OLLAMA_API_URL, {
      method: "POST",
      headers: headers,
      body: JSON.stringify({
        model: MODEL,
        messages: messages,
        stream: false,
        options: {
          temperature: 0.7,
          num_predict: 500,
        }
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Erro na API Ollama: ${response.status} - ${errorText}`);
    }

    const data = await response.json();
    return data.message.content;
  } catch (error) {
    console.error("Erro ao comunicar com Ollama:", error);
    throw error;
  }
};

export const generateFeedbackWithOllama = async (debateTopic, userName, chatHistory) => {
  try {
    const debateTranscript = chatHistory
      .map((msg) => {
        const speaker = msg.role === 'user' ? userName : 'Tuba';
        return `${speaker}: ${msg.parts[0].text}`;
      })
      .join('\n\n');

    const feedbackPrompt = `
[SOLICITA√á√ÉO DE FEEDBACK FINAL]

Voc√™ acabou de concluir um debate sobre "${debateTopic}" com ${userName}.

Aqui est√° o hist√≥rico completo do debate:

${debateTranscript}

---

Por favor, forne√ßa um feedback construtivo e detalhado sobre o desempenho de ${userName} neste debate. Inclua:

1. Pontos Fortes: O que ${userName} fez bem durante o debate?
2. √Åreas de Melhoria: Onde ${userName} poderia melhorar sua argumenta√ß√£o?
3. Qualidade dos Argumentos: Avalie a consist√™ncia e fundamenta√ß√£o dos argumentos apresentados.
4. Conclus√£o: Uma reflex√£o geral sobre o debate e sugest√µes para futuros debates.

Seja honesto, construtivo e encorajador. Use um tom amig√°vel e mantenha sua personalidade de tubar√£o! ü¶à
N√£o passe de 1 paragr√°fo.
    `.trim();

    const messages = [
      { role: "system", content: systemInstruction },
      { role: "user", content: feedbackPrompt }
    ];

    const headers = {
      "Content-Type": "application/json",
    };

    if (OLLAMA_API_KEY) {
      headers["Authorization"] = `Bearer ${OLLAMA_API_KEY}`;
    }

    const response = await fetch(OLLAMA_API_URL, {
      method: "POST",
      headers: headers,
      body: JSON.stringify({
        model: MODEL,
        messages: messages,
        stream: false,
        options: {
          temperature: 0.7,
          num_predict: 800,
        }
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Erro na API Ollama: ${response.status} - ${errorText}`);
    }

    const data = await response.json();
    return data.message.content;
  } catch (error) {
    console.error("Erro ao gerar feedback com Ollama:", error);
    throw error;
  }
};